\documentclass[12pt, letterpaper, twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{placeins}
\usepackage{booktabs}
\usepackage{multirow}


\title{Title}
\author{Author}
\date{Date}



\begin{document}
\maketitle


\section{Introduction}
In the latest few years, machine learning and deep learning have revolutionized the natural language processing field (NLP)\cite{khurana2023natural}. At the same time some fundamental ideas developed in NLP have been succesfuly applied to another type of language, the biological one: DNA, RNA and amminoacid sequences bringing to excellent results even in the complex task of protein structure prediction\cite{jumper2021highly, lin2022language}. One of these fundamental ideas is word embedding \cite{mikolov2013efficient} because it transforms words into points in space, therefore easy to process.
It is well known that sequence similarity do not always correspond to functional similarity \cite{kosloff2008sequence}.
Application of protein language model embeddings to downstream tasks was first demostrated by Bepler and Berger (2019) \cite{bepler2019learning}

\section{Methods}

In this section we describe the pipeline used to analyze the embeddings. As shown in Table \ref{tab:embedders}, the input length is different between the models as well as the output produced. We want to address the following problems: 1) compare different methods to join togheter the amminoacid-specific contextual representations in order to have a representation for the whole chunk and subsequently join togheter the representations of the chunks in order to have a representation for the whole protein; 2) find out if these representations reflect known properties of the proteins.

\subsection{Represent protein sequences as continuous vectors}
short description of the main characteristics of each embedder

\subsubsection{Prose \cite{bepler2021learning}}
The model structure is a multi-layer bidirectional Long Short Term Memory (bi-LSTM). Following the intuition that some aspect of proteins structure and semantic can never be discoverable by statistical sequence models alone, they came up with the idea of multi-task learning with strictural supervision. The learning tasks are: 1) classical masked language modeling task, 2) residue-residue contact prediction, 3) structural similarity prediction. This model ouperformed existing approaches in both transmembrane position labeling and phenotypes prediction of sequence variants.

\subsubsection{Alphafold \cite{jumper2021highly}}
This is the only approach that do not relay on a language model but is a combination of a bioinformatics and physical approaches.
It relys on large datasets of protein sequences that are similar enough to be aligned with high confidence but contain enough divergence to confidently infer statistical couplings between positions. It consists in two modules, Evoformer module and structure module. The Evoformer builds separate MSA and residue-pairwise embedding spaces. As it is structured, this system is not able to learn patterns across large-scale databases of possibly unrelated proteins \cite{bepler2021learning}.
In the experiments we used the ``single'' representation of the sequence, without the MSA and the templates.

Leggi altro paper sul genoma

\subsubsection{SeqVec \cite{heinzinger2019modeling} }
To build their model Heinzinger et al.\cite{heinzinger2019modeling} adapted the standar ELMo configuration \cite{peters-etal-2018-deep} to work with protein sequences modifying the number of tokens and the unroll steps. It is composed by 1 CharCNN and 2 LSTM-Layers. Given a protein sequence of arbitrary length it returns 3072 features for each residue derived by concatenating the outputs of the three layers of ELMo, each describing a token with a vector of length 1024. In order to obtain a smaller representation for each amminoacid we computed the mean of the three layers (as also suggested in the official repository). Given the architecture of the ELMo, these representation are contextual-dependent.


\subsubsection{Evolutionary scale modeling \cite{lin2023evolutionary}}
The underlying architecture is a BERT \cite{devlin2018bert} style encoder tarnsformer with modifications in the number of layers, number of attention heads, hidden size and feed forward hidden size. A further improvement is the use of Rotary Position Embedding \cite{su2021roformer} that allows to generalize beyond the context window it is trained on.
The main advantages compared to AlphaFold are the removal of the the need of multiple sequence alignment and an increasing up to two order of magnitude of  the speed of the prediction pipeline.
We used the version with 33 layers and 650B parameters (compare with alphafold) 


\subsection{Combining the (contextual) representations}
We tried four methods to join togheter the amminoacid embeddings in order to produce a fixed size embedding for the chunk: average, maximum, sum and principal component analysis (PCA). 

The same operator used to combine the amminoacid embeddings is also used to combine the embeddings of the chunks of the sequence.

Note on the contextal embedding: seqvec: contextual

\subsection{Comparison with known informations}
Given a set of embeddings of sequences we want to analyze their distribution in the embedding space comparing it with both the distance matrix produced during the multiple sequence alignment with Clustal Omega \cite{sievers2011fast} and higher level annotations as Gene Ontology \cite{10.1093/genetics/iyad031, ashburner2000gene}, UniProtKB Keywords and NCBI Taxonimy \cite{uniprot23}.

\subsubsection{Similarity between distance matrices \label{sec:similarity}}
In order to compare two distance matrices we performed an agglomerative clustering on both, the resulting tree is then cut at each level obtaining flat partitions of all possibles number of clusters. We performed a pairwise comparison of the partitions having the same number of clusters using the adjusted rand score \cite{hubert1985comparing}. The mean of these score, starting from two clusters up to $ \#elements - 1 $ clusters is called mean adjusted rand score (MARS). We compared different distance metrics as well as different methods to perform the hierarchical clustering.  

\subsubsection{Enrichment analysis}
The alignment distance matrix provide an evolutionary related distance between sequences \cite{SOFI202247}, we also wanted to analyze the properties of the embeddings at an higher level. The Gene Ontology (GO) describes our knowledge of the sequence with respect to: molecular function, cellular component and biological process; there ara also more specific controlled vocabulary as the UniProt Keywords and hierarchical classifications specific for sequences as the NCBI Taxonomy.

Whatever they are the sets of words to describe the sequences in our datasets, we want to build a distance between sequences among them. Given $A$ and $B$ the sets of annotations of two sequences we computed the distance in two possible ways: $$d1 = \frac{2 * |A \cap B|}{|A| + |B|} $$ $$d2 = \max\{ \frac{|A \cap B|}{|A|}, \frac{|A \cap B|}{|B|} \} $$

Both of them vary between $0$ and $1$, however $d1$ goes to 1 only when the two sets are equals while $d2$ goes to 1 also when one set is a subset of the other. After calculating one of these distance between all possible pair in the dataset we end up with a similarity matrix, that can be easly transformed in a distance matrix that is possible to comprare with the distance matrix derived from the distance between the embeddings using the MARS as described in subsection \ref{sec:similarity}.



\begin{table*}[htb]
\centering
\begin{tabular}{|l c c|} 
    \hline
    Name & input length (chunk) & embedding dimension  \\ 
    \hline
    embedding reproduction (rep)\cite{yang2018learned}       & 64    & 64 per chunk   \\
    seqvec \cite{heinzinger2019modeling} & 1024 & 1024 per amminoacid \\
    dnabert \cite{ji2021dnabert}                     & 512     & 768 per chunk \\
    prose   \cite{bepler2021learning}                   & 512   & 100 per amino acid   \\
    alphafold  \cite{jumper2021highly}                 & 1024   & 384 per ammino acid\\
    evolutionary scale modeling (esm2) \cite{lin2023evolutionary}   & 1024    & 1280 per ammino acid \\  
    \hline
\end{tabular}
\caption{Embedders used in the experiments, their maximum imput length and the dimension of the embedding produced.}
\label{tab:embedders}
\end{table*}




\begin{table*}[htb]
\centering
\begin{tabular}{|p{3cm} p{3cm} c c c |} 
    \hline
    Name & description & number of sequences & type & avg length   \\ 
    \hline
    hemoglobin &  hemoglobin for various organisms & 761 & amminoacids & 142  \\
    \hline
    mouse & mouse proteome     & 974 & amminoacids & 516 \\
    \hline
    bacterium & bacterium proteome  & 259 &  amminoacids & 427  \\
    \hline
    covid19 & covid19 complete genome & 77 & nucleotides & 29831  \\
    \hline
    meningitis & meningitis complete genome & 68 & nucleotides & 2240049 \\
    \hline
\end{tabular}
\caption{Datasets used in the experiments.}
\label{tab:dataset}
\end{table*}





\section{Results}
\subsection{Phylogenetic}



\begin{table*}[hbt]
    \begin{tabular}{llrrrrr}
        \toprule
         &  & dnabert & seqvec & prose & alphafold & esm \\
        combiner & dimensional PCA &  &  &  &  &  \\
        \midrule
        \multirow[t]{2}{*}{pca} & all & 0.076914 & 0.628944 & 0.438451 & 0.592638 & 0.655989 \\
         & default & 0.085592 & 0.462319 & 0.378802 & 0.440502 & 0.556404 \\
        \cline{1-7}
        \multirow[t]{2}{*}{average} & all & 0.150060 & 0.631618 & 0.436927 & 0.584828 & 0.654195 \\
         & default & 0.142402 & 0.468093 & 0.373694 & 0.428669 & 0.562981 \\
        \cline{1-7}
        \multirow[t]{2}{*}{sum} & all & 0.142098 & 0.632848 & 0.433519 & 0.584272 & 0.651882 \\
         & default & 0.140880 & 0.445979 & 0.365946 & 0.394104 & 0.530063 \\
        \cline{1-7}
        \multirow[t]{2}{*}{max} & all & 0.129600 & 0.648831 & 0.590373 & 0.458190 & 0.754162 \\
         & default & 0.133080 & 0.491197 & 0.460012 & 0.479472 & 0.628606 \\
        \cline{1-7}
        \bottomrule
        \end{tabular}
\caption{Emoglobin Phylogenetic results.}
\label{tab:emoglobin_phylo_results}
\end{table*}


\begin{table*}[hbt]
\begin{tabular}{llrrrrr}
    \toprule
     &  & dnabert & seqvec & prose & alphafold & esm \\
    combiner & dimensional PCA &  &  &  &  &  \\
    \midrule
    \multirow[t]{2}{*}{pca} & all & 0.029854 & 0.293372 & 0.263442 & 0.125250 & 0.278784 \\
     & default & 0.036239 & 0.291775 & 0.278305 & 0.090662 & 0.379791 \\
    \cline{1-7}
    \multirow[t]{2}{*}{average} & all & 0.008628 & 0.326197 & 0.306975 & 0.133504 & 0.272709 \\
     & default & 0.019080 & 0.325723 & 0.318528 & 0.204344 & 0.423269 \\
    \cline{1-7}
    \multirow[t]{2}{*}{sum} & all & 0.037913 & 0.326197 & 0.306975 & 0.133504 & 0.272709 \\
     & default & 0.051761 & 0.254459 & 0.309792 & 0.059609 & 0.205906 \\
    \cline{1-7}
    \multirow[t]{2}{*}{max} & all & 0.039009 & 0.396257 & 0.379740 & 0.053480 & 0.393414 \\
     & default & 0.020057 & 0.090137 & 0.432732 & 0.139505 & 0.187751 \\
    \cline{1-7}
    \bottomrule
\end{tabular}
\caption{Mouse Phylogenetic results.}
\label{tab:mouse_phylo_results}
\end{table*}


\begin{table*}[hbt]
\begin{tabular}{llrrrrr}
    \toprule
     &  & dnabert & seqvec & prose & alphafold & esm \\
    combiner & dimensional PCA &  &  &  &  &  \\
    \midrule
    \multirow[t]{2}{*}{pca} & all & -0.005314 & 0.064490 & 0.066430 & -0.003228 & 0.063098 \\
     & default & -0.004117 & 0.145555 & 0.075659 & 0.067033 & 0.109153 \\
    \cline{1-7}
    \multirow[t]{2}{*}{average} & all & -0.005307 & 0.085411 & 0.070529 & -0.000876 & 0.054788 \\
     & default & -0.003264 & 0.138811 & 0.092746 & 0.067431 & 0.106074 \\
    \cline{1-7}
    \multirow[t]{2}{*}{sum} & all & -0.005312 & 0.085411 & 0.070529 & -0.000876 & 0.054788 \\
     & default & 0.006822 & 0.075365 & 0.084606 & 0.006121 & 0.034575 \\
    \cline{1-7}
    \multirow[t]{2}{*}{max} & all & -0.005648 & 0.078479 & 0.107831 & -0.006607 & 0.080722 \\
     & default & -0.003445 & 0.009905 & 0.122704 & 0.027285 & 0.031228 \\
    \cline{1-7}
    \bottomrule
    \end{tabular}
\caption{bacterium Phylogenetic results.}
\label{tab:bacterium_phylo_results}
\end{table*}
    



\subsection{Enrichment}
\subsection{Projections?}
\subsection{Classification?}
\subsection{Pointwise representation similarity?}

\onecolumn
\newpage
\FloatBarrier
\bibliographystyle{plain}
\bibliography{bibliography}


\end{document}