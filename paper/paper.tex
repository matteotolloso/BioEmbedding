\documentclass[12pt, letterpaper, twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{placeins}


\title{Title}
\author{Author}
\date{Date}



\begin{document}
\maketitle


\section{Introduction}
In the latest few years, machine learning and deep learning have revolutionized the natural language processing field (NLP)\cite{khurana2023natural}. At the same time some fundamental ideas developed in NLP have been succesfuly applied to another type of language, the biological one: DNA, RNA and amminoacid sequences bringing to excellent results even in the complex task of protein structure prediction\cite{jumper2021highly, lin2022language}. One of these fundamental ideas is word embedding \cite{mikolov2013efficient} because it transforms words into points in space, therefore easy to process.
It is well known that sequence similarity do not always correspond to functional similarity \cite{kosloff2008sequence}.

\section{Methods}

In this section we describe the pipeline used to analyze the embeddings. As shown in Table \ref{tab:embedders}, the input length is different between the models as well as the output produced. We want to address the following problems: 1) compare different methods to join togheter the amminoacid-specific contextual representations in order to have a representation for the whole chunk and subsequently join togheter the representations of the chunks in order to have a representation for the whole protein; 2) find out if these representations reflect known properties of the proteins.

\subsection{Represent protein sequences as continuous vectors}
short description of the main characteristics of each embedder

\subsubsection{prose}
learning with strictural supervision

\subsubsection{alphafold}

- is a deep learning system not a language model

- predict residue-residue distances from sequence families and fold proteins based on the predicted distance constraints

- rely on large datasets of protein sequences that are similar enough to be aligned with high confidence but contain enough divergence to confidently infer statistical couplings between positions

- unable to learn patterns across large-scale databases of possibly unrelated proteins and have limited ability to draw on the increasing structure and function information available


\subsubsection{SeqVec}
To build their model Heinzinger et al.\cite{heinzinger2019modeling} adapted the standar ELMo configuration \cite{peters-etal-2018-deep} to work with protein sequences modifying the number of tokens and the unroll steps. It is composed by 1 CharCNN and 2 LSTM-Layers. Given a protein sequence of arbitrary length it returns 3072 features for each residue derived by concatenating the outputs of the three layers of ELMo, each describing a token with a vector of length 1024. In order to obtain a smaller representation for each amminoacid i computed the mean of the three layers (as also suggested in the official repository). Given the architecture of the ELMo, these representation are contextual-dependent.

\subsection{Combining the (contextual) representations}derived by concatenating the outputs of the three layers of ELMo, each describing a token with a vector of length 1024
We tried four methods to join togheter the amminoacid embeddings in order to produce a fixed size embedding for the chunk: average, maximum, sum and principal component analysis (PCA). 

The same operator used to combine the amminoacid embeddings is also used to combine the embeddings of the chunks of the sequence.

Note on the contextal embedding: seqvec: contextual

\subsection{Comparison with known informations}
Given a set of embeddings of sequences we want to analyze their distribution in the embedding space comparing it with both the distance matrix produced during the multiple sequence alignment with Clustal Omega \cite{sievers2011fast} and higher level annotations as Gene Ontology \cite{10.1093/genetics/iyad031, ashburner2000gene}, UniProtKB Keywords and NCBI Taxonimy \cite{uniprot23}.

\subsubsection{Similarity between distance matrices \label{sec:similarity}}
In order to compare two distance matrices we performed an agglomerative clustering on both, the resulting tree is then cut at each level obtaining flat partitions of all possibles number of clusters. We performed a pairwise comparison of the partitions having the same number of clusters using the adjusted rand score \cite{hubert1985comparing}. The mean of these score, starting from two clusters up to $ \#elements - 1 $ clusters is called mean adjusted rand score (MARS). We compared different distance metrics as well as different methods to perform the hierarchical clustering.  

\subsubsection{Enrichment analysis}
The alignment distance matrix provide an evolutionary related distance between sequences \cite{SOFI202247}, we also wanted to analyze the properties of the embeddings at an higher level. The Gene Ontology (GO) describes our knowledge of the sequence with respect to: molecular function, cellular component and biological process; there ara also more specific controlled vocabulary as the UniProt Keywords and hierarchical classifications specific for sequences as the NCBI Taxonomy.

Whatever they are the sets of words to describe the sequences in our datasets, we want to build a distance between sequences among them. Given $A$ and $B$ the sets of annotations of two sequences we computed the distance in two possible ways: $$d1 = \frac{2 * |A \cap B|}{|A| + |B|} $$ $$d2 = \max\{ \frac{|A \cap B|}{|A|}, \frac{|A \cap B|}{|B|} \} $$

Both of them vary between $0$ and $1$, however $d1$ goes to 1 only when the two sets are equals while $d2$ goes to 1 also when one set is a subset of the other. After calculating one of these distance between all possible pair in the dataset we end up with a similarity matrix, that can be easly transformed in a distance matrix that is possible to comprare with the distance matrix derived from the distance between the embeddings using the MARS as described in subsection \ref{sec:similarity}.


\onecolumn
\begin{table}[htb]
\centering
\begin{tabular}{|l c c|} 
    \hline
    Name & input length (chunk) & embedding dimension  \\ 
    \hline
    embedding reproduction (rep)\cite{yang2018learned}       & 64    & 64 per chunk   \\
    seqvec \cite{heinzinger2019modeling} & 1024 & 1024 per amminoacid \\
    dnabert \cite{ji2021dnabert}                     & 512     & 768 per chunk \\
    prose   \cite{bepler2021learning}                   & 512   & 100 per amino acid   \\
    alphafold  \cite{jumper2021highly}                 & 1024   & 384 per ammino acid\\
    evolutionary scale modeling (esm2) \cite{lin2022language}   & 1024    & 1280 per ammino acid \\  
    \hline
\end{tabular}
\caption{Embedders used in the experiments, their maximum imput length and the dimension of the embedding produced.}
\label{tab:embedders}
\end{table}
\twocolumn


\onecolumn
\begin{table}[htb]
\centering
\begin{tabular}{|p{3cm} p{3cm} c c c |} 
    \hline
    Name & description & number of sequences & type & avg length   \\ 
    \hline
    hemoglobin &  hemoglobin for various organisms & 761 & amminoacids & 142  \\
    \hline
    mouse & mouse proteome     & 974 & amminoacids & 516 \\
    \hline
    bacterium & bacterium proteome  & 259 &  amminoacids & 427  \\
    \hline
    covid19 & covid19 complete genome & 77 & nucleotides & 29831  \\
    \hline
    meningitis & meningitis complete genome & 68 & nucleotides & 2240049 \\
    \hline
\end{tabular}
\caption{Datasets used in the experiments.}
\label{tab:dataset}
\end{table}
\twocolumn




\section{Results}
\subsection{Phylogenetic}
\subsection{Enrichment}
\subsection{Projections?}
\subsection{Classification?}
\subsection{Pointwise representation similarity?}

\onecolumn
\newpage
\FloatBarrier
\bibliographystyle{plain}
\bibliography{bibliography}


\end{document}